{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ac1497",
   "metadata": {},
   "source": [
    "# Final use-case : School Failure Prediction\n",
    "\n",
    "This notebook will try to build a **predictive ai solution** to estimate a school failure for a given student.\n",
    "\n",
    "The system use a \"*Chain of Responsibiliy*\" pattern to pipeline the process. Each element of the chain is responsible of one process, then give the result to the other.\n",
    "\n",
    "This architecture allow to easily change or add process orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ce7660be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "\n",
    "from core.pipeline_core.pipeline_core import DataHandler, PipelineContext, PipelineOrchestrator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76fd24",
   "metadata": {},
   "source": [
    "*Refactorization*\n",
    "Handlers are removed from the book :\n",
    "- DataLoader is now in the core.handlers package (data_loader.py),\n",
    "- SensitiveDataHandler moved to core.handlers package (sensitive_data_handler.py)\n",
    "- MergerHandler moved to core.handlers package (merger_handler.py)\n",
    "- OutlierHandler moved to core.handlers package (outlier_handler.py)\n",
    "- ImputationHandler moved to core.handlers package (imputation_handler.py)\n",
    "- DataExportHandler moved to core.handlers package (data_export_handler.py),\n",
    "- ModelHandler moved to core.handlers package (model_handler.py)\n",
    "\n",
    "**Notes** :\n",
    "\n",
    "*Outlier handler :*\n",
    "Handle *outliers* using strategies :\n",
    "- IQR,\n",
    "- Isolation Forest\n",
    "Finally removes the entire line if one cols is ludicurious\n",
    "\n",
    "*NaN imputation :*\n",
    "Identity NaN values from the dataframe. Use different strategies for replacement :\n",
    "- AIImputation : Using regression to identify NaN (usefull for large dataframe)\n",
    "- SimpleImputer : Using either mean or median replacement strategy\n",
    "\n",
    "*Model Handler* :\n",
    "ModelHandler use *strategies*: \n",
    "- Logistic Regression - LR,\n",
    "- Random Forest Classifier - RF \n",
    "\n",
    "and use 4 base hypothesis : full_dataframe, no sensitive data, no_g1, no_g1_g2\n",
    "\n",
    "The pipeline scheme sets in a YAML config can be tuned to :\n",
    "- Add other algorithms,\n",
    "- Sets different hypothesis\n",
    "\n",
    "During training MLFlow is used to store parameters, artifacts and final models.\n",
    "\n",
    "## Move orchestration logic to YAML configuration\n",
    "Using configuration, we offer the possibility to handle steps dynamically. The following class read configuration and build the orchestration.\n",
    "\n",
    "**PipelineBuilder** moved to core.pipeline_core package (pipeline_builder.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4511dc97",
   "metadata": {},
   "source": [
    "## Orchestrator settings (@Deprecated)\n",
    "- Sets sources,\n",
    "- Sets sensitive datas,\n",
    "- Initiate orchestrator\n",
    "\n",
    "**Deprecated** after delegating to PiplelineBuilder, there's no need to manually configure the orchestrator.\n",
    "\n",
    "Kept only here for documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0a4f6ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.handlers.data_export_handler import DataExportHandler\n",
    "from core.handlers.data_loader import DataLoader\n",
    "from core.handlers.imputation_handler import ImputationHandler\n",
    "from core.handlers.merger_handler import MergerHandler\n",
    "from core.handlers.model_handler import ModelHandler\n",
    "from core.handlers.outlier_handler import OutlierHandler\n",
    "from core.handlers.sensitive_data_handler import SensitiveDataHandler\n",
    "from core.strategy_core.outliers_strategies import IsolationForestStrategy\n",
    "from core.strategy_core.imputation_strategies import AIImputationStrategy\n",
    "\n",
    "files_to_load = {\n",
    "    \"maths\": \"datas/student-mat.csv\",\n",
    "    \"por\": \"datas/student-por.csv\"\n",
    "}\n",
    "\n",
    "sensitives = [\n",
    "    \"romantic\", # No correlation\n",
    "    \"Dalc\", # Discriminant data, cannot be used\n",
    "    \"Walc\", # Discrimant data, cannot be used\n",
    "]\n",
    "\n",
    "# Make chain instances :\n",
    "# 1. Data processing chain\n",
    "loader = DataLoader(files_to_load=files_to_load)\n",
    "cleaner = SensitiveDataHandler(sensitive_columns=sensitives)\n",
    "merger = MergerHandler()\n",
    "\n",
    "# Sets one of the Outliers detection strategy (Isolation Forest)\n",
    "outlier_strategy = IsolationForestStrategy(contamination=0.01)\n",
    "outlier = OutlierHandler(strategy=outlier_strategy, target_columns=[\"studytime\", \"absences\", \"age\"])\n",
    "\n",
    "# Sets one of the Imputation Strategy\n",
    "imputer_strategy = AIImputationStrategy()\n",
    "imputer = ImputationHandler(imputer_strategy)\n",
    "\n",
    "exporter = DataExportHandler()\n",
    "\n",
    "# Instanciate Pipeline\n",
    "pipeline = (PipelineOrchestrator()\n",
    "    .add_handler(loader)\n",
    "    .add_handler(cleaner)\n",
    "    .add_handler(merger)\n",
    "    .add_handler(outlier)\n",
    "    .add_handler(imputer)\n",
    "    .add_handler(exporter)\n",
    ")\n",
    "\n",
    "# 2. Learning processing\n",
    "scenarii = [\n",
    "    (1, \"Full_Features\", []),\n",
    "    (2, \"No_Sensitive\", [\"romantic\", \"Dalc\", \"Walc\"]),\n",
    "    (3, \"No_Sensitive_No_G2\", [\"romantic\", \"Dalc\", \"Walc\", \"G2\"]),\n",
    "    (4, \"No_Sensitive_No_G1_G2\", [\"romantic\", \"Dalc\", \"Walc\", \"G1\", \"G2\"])\n",
    "]\n",
    "from core.strategy_core.training_strategies import LogisticRegressionStrategy\n",
    "from core.strategy_core.training_strategies import RandomForestStrategy\n",
    "# 2.1 From definitions add strategies needed\n",
    "for s_id, s_name, s_exclusions in scenarii:\n",
    "    for strategy_class in [LogisticRegressionStrategy, RandomForestStrategy]:\n",
    "        strategy = strategy_class(scenario_id=s_name, exclusions=s_exclusions)\n",
    "        model_handler = ModelHandler(strategy=strategy, scenario_label=s_name)\n",
    "        pipeline.add_handler(model_handler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae9019",
   "metadata": {},
   "source": [
    "## Run orchestrator\n",
    "\n",
    "Orchestrator is a Chain of Responsibilies. At the end of the chain, all processes are done.\n",
    "\n",
    "**Major update** : delegate chain assembly in a configuration file (see : pipeline_config.yml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "065d8f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-25 03:31:08.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcore.pipeline_core.pipeline_builder\u001b[0m:\u001b[36mbuild_from_yaml\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1müèóÔ∏è Building pipeline from Notebook classes...\u001b[0m\n",
      "\u001b[32m2025-12-25 03:31:08.166\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcore.pipeline_core.pipeline_builder\u001b[0m:\u001b[36m_get_class_from_anywhere\u001b[0m:\u001b[36m8\u001b[0m - \u001b[34m\u001b[1müîç Looking for class 'DataLoader' in module 'core.handlers.dataloader'\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'core.handlers.dataloader'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[147]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m context = PipelineContext()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Build pipeline from Notebook classes and YAML config\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m pipeline = \u001b[43mPipelineBuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_from_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpipeline_config.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morchestrator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/devspace/ia-simplon/final_notebook/core/pipeline_core/pipeline_builder.py:10\u001b[39m, in \u001b[36mbuild_from_yaml\u001b[39m\u001b[34m(config_path, orchestrator)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/devspace/ia-simplon/final_notebook/core/pipeline_core/pipeline_builder.py:17\u001b[39m, in \u001b[36m_instantiate_notebook_handler\u001b[39m\u001b[34m(name, config, params)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/devspace/ia-simplon/final_notebook/core/pipeline_core/pipeline_builder.py:9\u001b[39m, in \u001b[36m_get_class_from_anywhere\u001b[39m\u001b[34m(class_name, module_path)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_from_yaml\u001b[39m(config_path: \u001b[38;5;28mstr\u001b[39m, orchestrator):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(config_path, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         config = yaml.safe_load(f)\n\u001b[32m     11\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müèóÔ∏è Building pipeline from Notebook classes...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# 1. Data processing steps\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.7/lib/python3.13/importlib/__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'core.handlers.dataloader'"
     ]
    }
   ],
   "source": [
    "# Initialize context and orchestrator\n",
    "from core.pipeline_core.pipeline_builder import PipelineBuilder\n",
    "\n",
    "\n",
    "orchestrator = PipelineOrchestrator()\n",
    "context = PipelineContext()\n",
    "\n",
    "# Build pipeline from Notebook classes and YAML config\n",
    "pipeline = PipelineBuilder.build_from_yaml(\"pipeline_config.yaml\", orchestrator)\n",
    "\n",
    "# Run the pipeline\n",
    "try:\n",
    "    final_context = orchestrator.run(context)\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Pipeline failed: {e}\")\n",
    "finally:\n",
    "    print(\"üèÅ Pipeline execution ended.\")\n",
    "    \n",
    "    # Final report\n",
    "    print(\"\\n--- Merged datas overview ---\")\n",
    "    display(final_context.final_df.head())\n",
    "\n",
    "    print(\"\\n--- Execution stats ---\")\n",
    "    for step, duration in final_context.execution_time.items():\n",
    "        print(f\"{step:25} : {duration:.4f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad780f0",
   "metadata": {},
   "source": [
    "## Store the best run\n",
    "\n",
    "Prepare API exporting the best model (based uppon AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9736d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import mlflow\n",
    "\n",
    "# Get the best model from MLFlow tracking (based on AUC score)\n",
    "runs = mlflow.search_runs(order_by=[\"metrics.auc_score DESC\"])\n",
    "best_run_id = runs.iloc[0]['run_id']\n",
    "\n",
    "# Load and save the best model locally\n",
    "best_model = mlflow.sklearn.load_model(model_uri=f\"runs:/{best_run_id}/model\")\n",
    "joblib.dump(best_model, \"backend/models/student_model_auc_latest.joblib\")\n",
    "print(f\"‚úÖ Best model based on AUC score was saved successfully : {runs.iloc[0]['tags.mlflow.runName']}\")\n",
    "\n",
    "# Get the best model from MLFlow tracking (based on Accuracy score)\n",
    "runs = mlflow.search_runs(order_by=[\"metrics.accuracy DESC\"])\n",
    "best_run_id = runs.iloc[0]['run_id']\n",
    "\n",
    "# Load and save the best model locally\n",
    "best_model = mlflow.sklearn.load_model(model_uri=f\"runs:/{best_run_id}/model\")\n",
    "joblib.dump(best_model, \"backend/models/student_model_accuracy_latest.joblib\")\n",
    "print(f\"‚úÖ Best model based on Accuracy score was saved successfully : {runs.iloc[0]['tags.mlflow.runName']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
