{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ac1497",
   "metadata": {},
   "source": [
    "# Final use-case : School Failure Prediction\n",
    "\n",
    "This notebook will try to build a **predictive ai solution** to estimate a school failure for a given student.\n",
    "\n",
    "The system use a \"*Chain of Responsibiliy*\" pattern to pipeline the process. Each element of the chain is responsible of one process, then give the result to the other.\n",
    "\n",
    "This architecture allow to easily change or add process orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7660be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "\n",
    "from pipeline_core.pipeline_core import DataHandler, PipelineContext, PipelineOrchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e961f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_core.pipeline_core import PipelineContext\n",
    "\n",
    "\n",
    "class DataLoader(DataHandler):\n",
    "    \"\"\"\n",
    "    Handle data loading from csv\n",
    "    \"\"\"\n",
    "    def __init__(self, files_to_load):\n",
    "        self.files_to_load = files_to_load\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def process(self, context: PipelineContext) -> PipelineContext:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        from file_handling_core.file_manager import FileManager\n",
    "       \n",
    "\n",
    "        file_manager = FileManager()\n",
    "\n",
    "        for name, file in self.files_to_load.items():\n",
    "            df = file_manager.load_data(file)\n",
    "            if df is not None and len(df) > 0:\n",
    "                context.data_map[name] = df\n",
    "\n",
    "        return context\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af711513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_core.pipeline_core import PipelineContext\n",
    "\n",
    "\n",
    "class SensitiveDataHandler(DataHandler):\n",
    "    \"\"\"\n",
    "    Remove all sensible columns from sources before merge the two dataframes\n",
    "    See:\n",
    "        DataHandler abstract class\n",
    "    \"\"\"\n",
    "    def __init__(self, sensitive_columns: list):\n",
    "        super().__init__()\n",
    "        self.sensitive_columns = sensitive_columns\n",
    "\n",
    "    def process(self, context: PipelineContext) -> PipelineContext:\n",
    "        logger.info(f\"üîé Check sensitives columns in: {len(context.data_map)} sources\")\n",
    "\n",
    "        for name, df in context.data_map.items():\n",
    "            to_drop = [col for col in self.sensitive_columns if col in df.columns]\n",
    "\n",
    "            if to_drop:\n",
    "                context.data_map[name] = df.drop(columns=to_drop)\n",
    "                logger.debug(f\"‚ùå Remove {to_drop} from source: {name}\")\n",
    "            else:\n",
    "                logger.debug(f\"üå± No sensitive datas in source: {name}\")\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05936817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergerHandler(DataHandler):\n",
    "    \"\"\"\n",
    "    Merge context dataframe\n",
    "    Logs process in the context\n",
    "    \"\"\"\n",
    "    def process(self, context: PipelineContext) -> PipelineContext:\n",
    "        if not context.data_map:\n",
    "            raise ValueError(\"‚ùå MergerHandler : data_map is empty. Nothing to merge\")\n",
    "\n",
    "        source_names = list(context.data_map.keys())\n",
    "        logger.info(f\"üîÑ Merge sources : {source_names}\")\n",
    "\n",
    "        # 1. Check for columns consistance\n",
    "        first_df_cols = set(context.data_map[source_names[0]].columns)\n",
    "        for name in source_names[1:]:\n",
    "            current_cols = set(context.data_map[name].columns)\n",
    "            if first_df_cols != current_cols:\n",
    "                diff = first_df_cols.symmetric_difference(current_cols)\n",
    "                logger.warning(f\"‚ö†Ô∏è Diffrence between columns was detected {name}: {diff}\")\n",
    "                # Check if we can merge columuns \n",
    "\n",
    "        # 2. Prepare and merge\n",
    "        frames_to_concat = []\n",
    "        for name, df in context.data_map.items():\n",
    "            temp_df = df.copy()\n",
    "            temp_df['source_origin'] = name  # Ajout de la provenance\n",
    "            frames_to_concat.append(temp_df)\n",
    "\n",
    "        merged_df = pd.concat(frames_to_concat, ignore_index=True)\n",
    "        initial_count = len(merged_df)\n",
    "\n",
    "        # 3. Duplicates handling\n",
    "        # 'source_origin' ignored to identify real business duplicates\n",
    "        subset_cols = [col for col in merged_df.columns if col != 'source_origin']\n",
    "        context.final_df = merged_df.drop_duplicates(subset=subset_cols).reset_index(drop=True)\n",
    "        \n",
    "        duplicates_removed = initial_count - len(context.final_df)\n",
    "\n",
    "        # 4. Store logs in context (dict metadata/logs)\n",
    "        context.metadata['merger_report'] = {\n",
    "            'initial_rows': initial_count,\n",
    "            'final_rows': len(context.final_df),\n",
    "            'duplicates_removed': duplicates_removed,\n",
    "            'sources': source_names\n",
    "        }\n",
    "\n",
    "        logger.success(f\"‚úÖ Merge complete: {len(context.final_df)} rows kept ({duplicates_removed} duplicates remove).\")\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ee7e8b",
   "metadata": {},
   "source": [
    "## Outlier handler\n",
    "\n",
    "**Sujet** Identifie les valeurs \"ab√©rantes\" et utlise la strat√©gie de suppression de la ligne enti√®re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strategy_core.outliers_strategies import OutlierStrategy\n",
    "\n",
    "class OutlierHandler(DataHandler):\n",
    "    def __init__(self, strategy: OutlierStrategy, target_columns: list):\n",
    "        super().__init__()\n",
    "        self.strategy = strategy\n",
    "        self.target_columns = target_columns\n",
    "\n",
    "    def process(self, context: PipelineContext) -> PipelineContext:\n",
    "        \"\"\"\n",
    "        Identify ludicrous data and remove all row if found\n",
    "        \"\"\"\n",
    "        logger.info(\"üõ†Ô∏è Outliers detection running...\")\n",
    "        \n",
    "        final_df = context.final_df\n",
    "        if final_df is not None:\n",
    "            initial_count = len(final_df)\n",
    "            df_inlier = self.strategy.detect_and_clean(df=final_df, columns=self.target_columns)\n",
    "            removed = initial_count - len(df_inlier)\n",
    "\n",
    "            context.final_df = df_inlier\n",
    "\n",
    "            # Store metadatas\n",
    "            if \"outlier_reports\" not in context.metadata:\n",
    "                context.metadata['outlier_reports'] = {}\n",
    "            # Get the concrete strategy name\n",
    "            s_name = self.strategy.__class__.__name__\n",
    "            context.metadata[\"outlier_reports\"][s_name] = removed\n",
    "\n",
    "            logger.debug(f\"üóëÔ∏è {removed} outliers removed using {s_name}.\")\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f7af12",
   "metadata": {},
   "source": [
    "## NaN imputation\n",
    "\n",
    "**Sujet** : Identifier les valeurs manquantes et utiliser une strat√©gie pour remplacer\n",
    "\n",
    "On va utiliser une d√©tection intelligente \"regressive\" pour isoler les donn√©es manquantes et les remplacer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea724868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_core.pipeline_core import PipelineContext\n",
    "from strategy_core.imputation_strategies import ImputationStrategy\n",
    "\n",
    "class ImputationHandler(DataHandler):\n",
    "    \"\"\"\n",
    "    Identify and impute missing values\n",
    "    \"\"\"\n",
    "    def __init__(self, strategy: ImputationStrategy):\n",
    "        self.strategy = strategy\n",
    "        super().__init__()\n",
    "\n",
    "    def process(self, context: PipelineContext) -> PipelineContext:\n",
    "\n",
    "        if context.final_df is None:\n",
    "            logger.error(\"‚ùå SmartImputationHandler : final_df is empty. This handler must be place AFTER MergerHandler.\")\n",
    "            return context\n",
    "        \n",
    "        df = context.final_df\n",
    "\n",
    "        # 1. Automatic NaN columns detection\n",
    "        nan_report = df.isna().sum()\n",
    "        cols_with_nan = nan_report[nan_report > 0].index.tolist()\n",
    "\n",
    "        # Only numercial columns are kept\n",
    "        target_cols = [c for c in cols_with_nan if pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "        if not target_cols:\n",
    "            logger.info(\"‚úÖ No missing datas detected in the dataframe\")\n",
    "            return context\n",
    "        \n",
    "\n",
    "        logger.info(\"üõ†Ô∏è Smart NaN imputation running...\")\n",
    "\n",
    "        # Applying strategy\n",
    "        initial_nan_count = df[target_cols].isna().sum().sum()\n",
    "        context.final_df = self.strategy.apply(df, target_cols)\n",
    "\n",
    "        # Logging and metadatas\n",
    "        context.logs[\"imputation_report\"] = {\n",
    "            \"fixed_columns\": target_cols,\n",
    "            \"total_values_filled\": int(initial_nan_count)\n",
    "        }\n",
    "\n",
    "        logger.success(f\"‚ú® {initial_nan_count} successfuly missing datas processed\")\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d5f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from file_handling_core.file_manager import FileManager\n",
    "\n",
    "class DataExportHandler(DataHandler):\n",
    "    \"\"\"\n",
    "    Save dataframe using FileHandler\n",
    "    Comes after cleaning and merging\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir: str = \"outputs/data_processed\"):\n",
    "        super().__init__()\n",
    "        self.output_dir = output_dir\n",
    "        self.file_manager = FileManager()\n",
    "        \n",
    "        # Create folder if not exists\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "            logger.info(f\"üìÅ Folder created: {self.output_dir}\")\n",
    "\n",
    "    def process(self, context: PipelineContext) -> PipelineContext:\n",
    "        if context.final_df is None or context.final_df.empty:\n",
    "            logger.warning(\"‚ö†Ô∏è DataExportHandler: No data to save (df is empty).\")\n",
    "            return context\n",
    "\n",
    "        # Filename generation: student_JJMMAAAA_HHMMss_processed.csv\n",
    "        timestamp = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "        file_name = f\"student_{timestamp}_processed.csv\"\n",
    "        full_path = os.path.join(self.output_dir, file_name)\n",
    "\n",
    "        try:\n",
    "            logger.info(f\"üíæ Try to save to {full_path}\")\n",
    "            self.file_manager.save_processed_data(context.final_df, full_path)\n",
    "            \n",
    "            # Add log to context\n",
    "            context.metadata['export_path'] = full_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Data export failed: {e}\")\n",
    "            raise # Stop here if failed\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eebdb7",
   "metadata": {},
   "source": [
    "## Learning handler\n",
    "Using strategies, handler will run Logistic Regression (LR) or Random Forest Classifier (RF) with 4 hypothesis.\n",
    "During training, MLFlow stores metrics, artifacts and finally store the most powerfull model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strategy_core.training_strategies import TrainingStrategy\n",
    "\n",
    "class ModelHandler(DataHandler):\n",
    "    def __init__(self, strategy: TrainingStrategy, scenario_label:str):\n",
    "        \"\"\"\n",
    "        Initiate Training Model\n",
    "        Params:\n",
    "            strategy: TrainingStrategy one of the strategy to use\n",
    "            scenario_label: str - Scenario to store into MLFlow tracking\n",
    "        \"\"\"\n",
    "        self.strategy = strategy\n",
    "        self.scenario_label = scenario_label\n",
    "\n",
    "    def process(self, context: PipelineContext) -> PipelineContext:\n",
    "        logger.info(f\"üöÄ Training launching: {self.scenario_label}\")\n",
    "\n",
    "        if context.final_df is not None:\n",
    "            self.strategy.execute(context.final_df, self.scenario_label)\n",
    "            return context\n",
    "        else:\n",
    "            logger.error(\"‚ùå dataframe is none. Process interrupted!\")\n",
    "            raise Exception(\"Dataframe is none or empty. Training was interrupted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4511dc97",
   "metadata": {},
   "source": [
    "## Orchestrator settings\n",
    "- Sets sources,\n",
    "- Sets sensitive datas,\n",
    "- Initiate orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f6ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strategy_core.outliers_strategies import IsolationForestStrategy\n",
    "from strategy_core.imputation_strategies import AIImputationStrategy\n",
    "\n",
    "files_to_load = {\n",
    "    \"maths\": \"datas/student-mat.csv\",\n",
    "    \"por\": \"datas/student-por.csv\"\n",
    "}\n",
    "\n",
    "sensitives = [\n",
    "    \"romantic\", # No correlation\n",
    "    \"Dalc\", # Discriminant data, cannot be used\n",
    "    \"Walc\", # Discrimant data, cannot be used\n",
    "]\n",
    "\n",
    "# Make chain instances :\n",
    "# 1. Data processing chain\n",
    "loader = DataLoader(files_to_load=files_to_load)\n",
    "cleaner = SensitiveDataHandler(sensitive_columns=sensitives)\n",
    "merger = MergerHandler()\n",
    "\n",
    "# Sets one of the Outliers detection strategy (Isolation Forest)\n",
    "outlier_strategy = IsolationForestStrategy(contamination=0.01)\n",
    "outlier = OutlierHandler(strategy=outlier_strategy, target_columns=[\"studytime\", \"absences\", \"age\"])\n",
    "\n",
    "# Sets one of the Imputation Strategy\n",
    "imputer_strategy = AIImputationStrategy()\n",
    "imputer = ImputationHandler(imputer_strategy)\n",
    "\n",
    "exporter = DataExportHandler()\n",
    "\n",
    "# Instanciate Pipeline\n",
    "pipeline = (PipelineOrchestrator()\n",
    "    .add_handler(loader)\n",
    "    .add_handler(cleaner)\n",
    "    .add_handler(merger)\n",
    "    .add_handler(outlier)\n",
    "    .add_handler(imputer)\n",
    "    .add_handler(exporter)\n",
    ")\n",
    "\n",
    "# 2. Learning processing\n",
    "scenarii = [\n",
    "    (1, \"Full_Features\", []),\n",
    "    (2, \"No_Sensitive\", [\"romantic\", \"Dalc\", \"Walc\"]),\n",
    "    (3, \"No_Sensitive_No_G2\", [\"romantic\", \"Dalc\", \"Walc\", \"G2\"]),\n",
    "    (4, \"No_Sensitive_No_G1_G2\", [\"romantic\", \"Dalc\", \"Walc\", \"G1\", \"G2\"])\n",
    "]\n",
    "from strategy_core.training_strategies import LogisticRegressionStrategy\n",
    "from strategy_core.training_strategies import RandomForestStrategy\n",
    "# 2.1 From definitions add strategies needed\n",
    "for s_id, s_name, s_exclusions in scenarii:\n",
    "    for strategy_class in [LogisticRegressionStrategy, RandomForestStrategy]:\n",
    "        strategy = strategy_class(scenario_id=s_name, exclusions=s_exclusions)\n",
    "        model_handler = ModelHandler(strategy=strategy, scenario_label=s_name)\n",
    "        pipeline.add_handler(model_handler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae9019",
   "metadata": {},
   "source": [
    "## Run orchestrator\n",
    "\n",
    "Orchestrator is a Chain of Responsibilies. At the end of the chain, all processes are done.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d8f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-17 15:55:58.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline_core.pipeline_core\u001b[0m:\u001b[36mhandle\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mStep DataLoader started...\u001b[0m\n",
      "\u001b[32m2025-12-17 15:55:58.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfile_handling_core.file_manager\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1müìÑ Successfuly loaded data from: datas/student-mat.csv\u001b[0m\n",
      "\u001b[32m2025-12-17 15:55:58.217\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mfile_handling_core.file_manager\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m38\u001b[0m - \u001b[31m\u001b[1m‚ùå Data loading error; [Errno 2] No such file or directory: 'datas/student-port.csv'\u001b[0m\n",
      "\u001b[32m2025-12-17 15:55:58.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline_core.pipeline_core\u001b[0m:\u001b[36mhandle\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mStep SensitiveDataHandler started...\u001b[0m\n",
      "\u001b[32m2025-12-17 15:55:58.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1müîé Check sensitives columns in: 1 sources\u001b[0m\n",
      "\u001b[32m2025-12-17 15:55:58.219\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess\u001b[0m:\u001b[36m24\u001b[0m - \u001b[34m\u001b[1müå± No sensitive datas in source: maths\u001b[0m\n",
      "\u001b[32m2025-12-17 15:55:58.219\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[32m\u001b[1m‚òë Pipeline fully executed\u001b[0m\n",
      "\u001b[32m2025-12-17 15:55:58.220\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[31m\u001b[1m‚ùå Pipeline failed: 'NoneType' object has no attribute 'head'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data shape: (395, 1)\n",
      "\n",
      "Columns: Index(['school;sex;age;address;famsize;Pstatus;Medu;Fedu;Mjob;Fjob;reason;guardian;traveltime;studytime;failures;schoolsup;famsup;paid;activities;nursery;higher;internet;romantic;famrel;freetime;goout;Dalc;Walc;health;absences;G1;G2;G3'], dtype='object')\n",
      "\n",
      "CRows: 395\n",
      "\n",
      "Types:\n",
      " school;sex;age;address;famsize;Pstatus;Medu;Fedu;Mjob;Fjob;reason;guardian;traveltime;studytime;failures;schoolsup;famsup;paid;activities;nursery;higher;internet;romantic;famrel;freetime;goout;Dalc;Walc;health;absences;G1;G2;G3    object\n",
      "dtype: object\n",
      "\n",
      "--- Merged datas overview ---\n"
     ]
    }
   ],
   "source": [
    "# Initialize context\n",
    "context = PipelineContext()\n",
    "\n",
    "# Run the pipeline\n",
    "try:\n",
    "    pipeline.configure_pipeline() # Configure the pipeline\n",
    "    final_context = pipeline.run(context)\n",
    "    logger.success(\"‚òë Pipeline fully executed\")\n",
    "\n",
    "    # Final report\n",
    "    print(\"\\n--- Merged datas overview ---\")\n",
    "    display(final_context.final_df.head())\n",
    "\n",
    "    print(\"\\n--- Execution stats ---\")\n",
    "    for step, duration in final_context.execution_time.items():\n",
    "        print(f\"{step:25} : {duration:.4f}s\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Pipeline failed: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
